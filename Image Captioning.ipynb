{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Image Captioning.ipynb",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Архитектура модели, создающей описания изображения на английском языке, состоит из 3 частей: сверточной нейронной сети, модуля внимания и рекуррентной нейронной сети. В качестве CNN была выбрана предобученная сеть EfficientNet-B7, поскольку она является одной из самых эффективных сверточных нейросетей последнего поколения. При этом у нее были убраны два последних слоя – линейный слой и слой пулинга, а веса самой сети при обучении заморожены. Поверх данной сети был добавлен обучаемый сверточный слой с целью дообучения модели на выбранном наборе данных и придания ей большей гибкости. В качестве модуля внимания был выбран механизм Bahdanau Attention, так как он является достаточно распространенным для решаемой задачи и довольно прост в реализации. В качестве рекуррентной нейронной сети был использован стандартный модуль LSTM."
      ],
      "metadata": {
        "id": "tH6LFLMCw8sv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Импорт библиотек и подготовка данных"
      ],
      "metadata": {
        "id": "v3i_t0Vnw05F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torchvision.transforms as T\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import nltk\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:20:48.321584Z",
          "iopub.execute_input": "2022-04-28T07:20:48.321872Z",
          "iopub.status.idle": "2022-04-28T07:21:00.259943Z",
          "shell.execute_reply.started": "2022-04-28T07:20:48.321797Z",
          "shell.execute_reply": "2022-04-28T07:21:00.25919Z"
        },
        "trusted": true,
        "id": "j_ZMGxi1w05Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self,freq_threshold):\n",
        "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
        "        self.stoi = {v:k for k,v in self.itos.items()}        \n",
        "        self.freq_threshold = freq_threshold\n",
        "        \n",
        "    def __len__(self): return len(self.itos)\n",
        "    \n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n",
        "    \n",
        "    def build_vocab(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = 4\n",
        "        \n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenize(sentence):\n",
        "                frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "    \n",
        "    def numericalize(self,text):\n",
        "        tokenized_text = self.tokenize(text)\n",
        "        return [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:00.261739Z",
          "iopub.execute_input": "2022-04-28T07:21:00.261985Z",
          "iopub.status.idle": "2022-04-28T07:21:00.271578Z",
          "shell.execute_reply.started": "2022-04-28T07:21:00.261951Z",
          "shell.execute_reply": "2022-04-28T07:21:00.27092Z"
        },
        "trusted": true,
        "id": "OqlXhjyGw05T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(Dataset):\n",
        "\n",
        "    def __init__(self,root_dir,captions_file,transform=None,freq_threshold=5):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(captions_file)\n",
        "        self.transform = transform\n",
        "        self.imgs = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocab(self.captions.tolist())\n",
        "        \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        caption = self.captions[idx]\n",
        "        img_name = self.imgs[idx]\n",
        "        img_location = os.path.join(self.root_dir,img_name)\n",
        "        img = Image.open(img_location).convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        caption_vec = []\n",
        "        caption_vec += [self.vocab.stoi[\"<SOS>\"]]\n",
        "        caption_vec += self.vocab.numericalize(caption)\n",
        "        caption_vec += [self.vocab.stoi[\"<EOS>\"]]\n",
        "        \n",
        "        return img, torch.tensor(caption_vec)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:00.27308Z",
          "iopub.execute_input": "2022-04-28T07:21:00.273683Z",
          "iopub.status.idle": "2022-04-28T07:21:00.283726Z",
          "shell.execute_reply.started": "2022-04-28T07:21:00.273646Z",
          "shell.execute_reply": "2022-04-28T07:21:00.283034Z"
        },
        "trusted": true,
        "id": "NitHuNLPw05U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetTest(Dataset):\n",
        "    \n",
        "    def __init__(self, root_dir, caption_file, transform=None, freq_threshold=5):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.df = pd.read_csv(caption_file)\n",
        "        self.imgs = os.listdir(self.root_dir)\n",
        "        self.captions = self.df[\"caption\"]\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocab(self.captions.tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.imgs[idx]\n",
        "        img_location = os.path.join(self.root_dir, img_name)\n",
        "        img = Image.open(img_location).convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:00.286032Z",
          "iopub.execute_input": "2022-04-28T07:21:00.286564Z",
          "iopub.status.idle": "2022-04-28T07:21:00.297286Z",
          "shell.execute_reply.started": "2022-04-28T07:21:00.28653Z",
          "shell.execute_reply": "2022-04-28T07:21:00.296614Z"
        },
        "trusted": true,
        "id": "iD_xhrVuw05W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loader(dataset, batch_size, shuffle=False, num_workers=1):\n",
        "    \n",
        "    data_loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "35gUqDaAyavg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = T.Compose([\n",
        "    T.Resize((224,224)),\n",
        "    T.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:00.299982Z",
          "iopub.execute_input": "2022-04-28T07:21:00.300582Z",
          "iopub.status.idle": "2022-04-28T07:21:00.309258Z",
          "shell.execute_reply.started": "2022-04-28T07:21:00.300547Z",
          "shell.execute_reply": "2022-04-28T07:21:00.308474Z"
        },
        "trusted": true,
        "id": "M06lg9Tyw05a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_kxOeyd3xVBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(inp, title=None):\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:00.310606Z",
          "iopub.execute_input": "2022-04-28T07:21:00.311106Z",
          "iopub.status.idle": "2022-04-28T07:21:00.317803Z",
          "shell.execute_reply.started": "2022-04-28T07:21:00.311071Z",
          "shell.execute_reply": "2022-04-28T07:21:00.317101Z"
        },
        "trusted": true,
        "id": "hCV9Kpc8w05c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset(\n",
        "    root_dir = 'coco-2017-dataset/coco2017/train2017',\n",
        "    captions_file = 'cococaptions/train_captions.txt',\n",
        "    transform=transforms\n",
        ")\n",
        "\n",
        "img, caps = dataset[0]\n",
        "show_image(img,\"Image\")\n",
        "print(\"Token:\",caps)\n",
        "print(\"Sentence:\")\n",
        "print([dataset.vocab.itos[token] for token in caps.tolist()])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:00.320258Z",
          "iopub.execute_input": "2022-04-28T07:21:00.320507Z",
          "iopub.status.idle": "2022-04-28T07:21:26.472555Z",
          "shell.execute_reply.started": "2022-04-28T07:21:00.320477Z",
          "shell.execute_reply": "2022-04-28T07:21:26.471805Z"
        },
        "trusted": true,
        "id": "wVaN2sSaw05d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CapsCollate:\n",
        "     \n",
        "    def __init__(self,pad_idx,batch_first=False):\n",
        "        self.pad_idx = pad_idx\n",
        "        self.batch_first = batch_first\n",
        "    \n",
        "    def __call__(self,batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs,dim=0)\n",
        "        \n",
        "        targets = [item[1] for item in batch]\n",
        "        targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n",
        "        return imgs,targets"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:26.473549Z",
          "iopub.execute_input": "2022-04-28T07:21:26.474326Z",
          "iopub.status.idle": "2022-04-28T07:21:26.480915Z",
          "shell.execute_reply.started": "2022-04-28T07:21:26.474287Z",
          "shell.execute_reply": "2022-04-28T07:21:26.480256Z"
        },
        "trusted": true,
        "id": "ZIMWQy8tw05e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4\n",
        "NUM_WORKER = 1\n",
        "\n",
        "pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "data_loader = DataLoader(\n",
        "    dataset=dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKER,\n",
        "    shuffle=True,\n",
        "    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True)\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:26.481991Z",
          "iopub.execute_input": "2022-04-28T07:21:26.482392Z",
          "iopub.status.idle": "2022-04-28T07:21:26.494305Z",
          "shell.execute_reply.started": "2022-04-28T07:21:26.482353Z",
          "shell.execute_reply": "2022-04-28T07:21:26.493546Z"
        },
        "trusted": true,
        "id": "hxcseA2yw05f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(data_loader)\n",
        "batch = next(dataiter)\n",
        "images, captions = batch\n",
        "\n",
        "for i in range(BATCH_SIZE):\n",
        "    img,cap = images[i],captions[i]\n",
        "    caption_label = [dataset.vocab.itos[token] for token in cap.tolist()]\n",
        "    eos_index = caption_label.index('<EOS>')\n",
        "    caption_label = caption_label[1:eos_index]\n",
        "    caption_label = ' '.join(caption_label)                      \n",
        "    show_image(img,caption_label)\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:26.498161Z",
          "iopub.execute_input": "2022-04-28T07:21:26.49871Z",
          "iopub.status.idle": "2022-04-28T07:21:27.642294Z",
          "shell.execute_reply.started": "2022-04-28T07:21:26.498678Z",
          "shell.execute_reply": "2022-04-28T07:21:27.641463Z"
        },
        "trusted": true,
        "id": "KIrBS1A1w05i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loader(dataset,batch_size,shuffle=False,num_workers=1):\n",
        "    \n",
        "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "    collate_fn = CapsCollate(pad_idx=pad_idx,batch_first=True)\n",
        "\n",
        "    data_loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    return data_loader"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:27.648997Z",
          "iopub.execute_input": "2022-04-28T07:21:27.649192Z",
          "iopub.status.idle": "2022-04-28T07:21:27.655929Z",
          "shell.execute_reply.started": "2022-04-28T07:21:27.649164Z",
          "shell.execute_reply": "2022-04-28T07:21:27.655251Z"
        },
        "trusted": true,
        "id": "8mvjkOX3w05j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Проверка"
      ],
      "metadata": {
        "id": "K5VMlKKrw05k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 256\n",
        "NUM_WORKER = 4\n",
        "\n",
        "transforms = T.Compose([\n",
        "    T.Resize(226),                     \n",
        "    T.RandomCrop(224),                 \n",
        "    T.ToTensor(),                               \n",
        "    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "data_loader = get_data_loader(\n",
        "    dataset=dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKER,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "vocab_size = len(dataset.vocab)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:27.673071Z",
          "iopub.execute_input": "2022-04-28T07:21:27.673324Z",
          "iopub.status.idle": "2022-04-28T07:21:50.27802Z",
          "shell.execute_reply.started": "2022-04-28T07:21:27.67329Z",
          "shell.execute_reply": "2022-04-28T07:21:50.277285Z"
        },
        "trusted": true,
        "id": "bjqnyMwhw05l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(img, title=None):\n",
        "\n",
        "    img[0] = img[0] * 0.229\n",
        "    img[1] = img[1] * 0.224 \n",
        "    img[2] = img[2] * 0.225 \n",
        "    img[0] += 0.485 \n",
        "    img[1] += 0.456 \n",
        "    img[2] += 0.406\n",
        "    \n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "    \n",
        "    \n",
        "    plt.imshow(img)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:50.27955Z",
          "iopub.execute_input": "2022-04-28T07:21:50.279834Z",
          "iopub.status.idle": "2022-04-28T07:21:50.286436Z",
          "shell.execute_reply.started": "2022-04-28T07:21:50.279797Z",
          "shell.execute_reply": "2022-04-28T07:21:50.285571Z"
        },
        "trusted": true,
        "id": "2Go0wNeiw05m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_val = Dataset(\n",
        "    root_dir = \"coco-2017-dataset/coco2017/val2017\",\n",
        "    captions_file = \"cococaptions/val_captions.txt\",\n",
        "    transform=transforms\n",
        ")\n",
        "\n",
        "data_loader_val = get_data_loader(\n",
        "    dataset=dataset_val,\n",
        "    batch_size=1,\n",
        "    num_workers=NUM_WORKER,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:50.288325Z",
          "iopub.execute_input": "2022-04-28T07:21:50.288635Z",
          "iopub.status.idle": "2022-04-28T07:21:51.372317Z",
          "shell.execute_reply.started": "2022-04-28T07:21:50.288597Z",
          "shell.execute_reply": "2022-04-28T07:21:51.371569Z"
        },
        "trusted": true,
        "id": "CHddLOGJw05n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset.vocab)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:51.373728Z",
          "iopub.execute_input": "2022-04-28T07:21:51.373989Z",
          "iopub.status.idle": "2022-04-28T07:21:51.379739Z",
          "shell.execute_reply.started": "2022-04-28T07:21:51.373956Z",
          "shell.execute_reply": "2022-04-28T07:21:51.378927Z"
        },
        "trusted": true,
        "id": "N5T4gFvxw05n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test =  DatasetTest(\n",
        "    root_dir = \"coco-2017-dataset/coco2017/test2017\",\n",
        "    caption_file = \"cococaptions/val_captions.txt\",\n",
        "    transform=transforms\n",
        ")\n",
        "\n",
        "data_loader_test = get_data_loader(\n",
        "    dataset=dataset_val,\n",
        "    batch_size=1,\n",
        "    num_workers=NUM_WORKER,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:51.381254Z",
          "iopub.execute_input": "2022-04-28T07:21:51.381764Z",
          "iopub.status.idle": "2022-04-28T07:21:53.395934Z",
          "shell.execute_reply.started": "2022-04-28T07:21:51.381696Z",
          "shell.execute_reply": "2022-04-28T07:21:53.395184Z"
        },
        "trusted": true,
        "id": "LrbnA66pw05o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Архитектура модели"
      ],
      "metadata": {
        "id": "GDisoyBNw05o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        effnet = models.efficientnet_b7(pretrained=True)\n",
        "        for param in effnet.parameters():\n",
        "            param.requires_grad_(False)\n",
        "\n",
        "        modules = list(effnet.children())[:-2]\n",
        "        self.effnet = nn.Sequential(*modules)\n",
        "        self.conv = nn.Sequential(nn.Conv2d(2560, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
        "                                  nn.BatchNorm2d(2048, eps=0.001, momentum=0.01, affine=True, track_running_stats=True),\n",
        "                                  nn.SiLU(inplace=True))\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.effnet(images)\n",
        "        features = self.conv(features)\n",
        "        features = features.permute(0, 2, 3, 1)\n",
        "        features = features.view(features.size(0), -1, features.size(-1))\n",
        "        return features"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:53.851314Z",
          "iopub.execute_input": "2022-04-28T07:21:53.851594Z",
          "iopub.status.idle": "2022-04-28T07:21:53.858896Z",
          "shell.execute_reply.started": "2022-04-28T07:21:53.851559Z",
          "shell.execute_reply": "2022-04-28T07:21:53.858235Z"
        },
        "trusted": true,
        "id": "3DCmQWqkw05q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim,decoder_dim,attention_dim):\n",
        "        super(Attention, self).__init__()        \n",
        "        self.attention_dim = attention_dim        \n",
        "        self.W = nn.Linear(decoder_dim,attention_dim)\n",
        "        self.U = nn.Linear(encoder_dim,attention_dim)        \n",
        "        self.A = nn.Linear(attention_dim,1)\n",
        "        \n",
        "    def forward(self, features, hidden_state):\n",
        "        u_hs = self.U(features)\n",
        "        w_ah = self.W(hidden_state)        \n",
        "        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1))        \n",
        "        attention_scores = self.A(combined_states)\n",
        "        attention_scores = attention_scores.squeeze(2)        \n",
        "        alpha = F.softmax(attention_scores,dim=1)        \n",
        "        attention_weights = features * alpha.unsqueeze(2)\n",
        "        attention_weights = attention_weights.sum(dim=1)\n",
        "        \n",
        "        return alpha, attention_weights"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:53.860211Z",
          "iopub.execute_input": "2022-04-28T07:21:53.860654Z",
          "iopub.status.idle": "2022-04-28T07:21:53.873263Z",
          "shell.execute_reply.started": "2022-04-28T07:21:53.860615Z",
          "shell.execute_reply": "2022-04-28T07:21:53.872426Z"
        },
        "trusted": true,
        "id": "aXc_w0Tow05q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.attention_dim = attention_dim\n",
        "        self.decoder_dim = decoder_dim        \n",
        "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
        "        self.attention = Attention(encoder_dim,decoder_dim,attention_dim)        \n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n",
        "        self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True)\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)        \n",
        "        self.fcn = nn.Linear(decoder_dim,vocab_size)\n",
        "        self.drop = nn.Dropout(drop_prob)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        \n",
        "        embeds = self.embedding(captions)\n",
        "        \n",
        "        h, c = self.init_hidden_state(features)\n",
        "        seq_length = len(captions[0])-1\n",
        "        batch_size = captions.size(0)\n",
        "        num_features = features.size(1)        \n",
        "        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n",
        "                \n",
        "        for s in range(seq_length):\n",
        "            alpha,context = self.attention(features, h)\n",
        "            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n",
        "            h, c = self.lstm_cell(lstm_input, (h, c))                    \n",
        "            output = self.fcn(self.drop(h))            \n",
        "            preds[:,s] = output\n",
        "            alphas[:,s] = alpha  \n",
        "        \n",
        "        return preds, alphas\n",
        "    \n",
        "    def generate_caption(self,features,max_len=20,vocab=None):\n",
        "\n",
        "        batch_size = features.size(0)\n",
        "        h, c = self.init_hidden_state(features)\n",
        "        alphas = []     \n",
        "        word = torch.tensor(vocab.stoi['<SOS>']).view(1,-1).to(device)\n",
        "        embeds = self.embedding(word)\n",
        "        captions = []\n",
        "        \n",
        "        for i in range(max_len):\n",
        "            alpha,context = self.attention(features, h)\n",
        "            alphas.append(alpha.cpu().detach().numpy())\n",
        "            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n",
        "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
        "            output = self.fcn(self.drop(h))\n",
        "            output = output.view(batch_size,-1)        \n",
        "            predicted_word_idx = output.argmax(dim=1)\n",
        "            captions.append(predicted_word_idx.item())\n",
        "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
        "                break\n",
        "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))        \n",
        "       \n",
        "        return [vocab.itos[idx] for idx in captions],alphas\n",
        "    \n",
        "    \n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:53.874534Z",
          "iopub.execute_input": "2022-04-28T07:21:53.874949Z",
          "iopub.status.idle": "2022-04-28T07:21:53.895841Z",
          "shell.execute_reply.started": "2022-04-28T07:21:53.874913Z",
          "shell.execute_reply": "2022-04-28T07:21:53.895113Z"
        },
        "trusted": true,
        "id": "d6VksfUjw05r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderCNN()\n",
        "        self.decoder = DecoderRNN(\n",
        "            embed_size=embed_size,\n",
        "            vocab_size = len(dataset.vocab),\n",
        "            attention_dim=attention_dim,\n",
        "            encoder_dim=encoder_dim,\n",
        "            decoder_dim=decoder_dim\n",
        "        )\n",
        "        \n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoder(images)\n",
        "        outputs = self.decoder(features, captions)\n",
        "        return outputs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:53.897098Z",
          "iopub.execute_input": "2022-04-28T07:21:53.89751Z",
          "iopub.status.idle": "2022-04-28T07:21:53.908084Z",
          "shell.execute_reply.started": "2022-04-28T07:21:53.897472Z",
          "shell.execute_reply": "2022-04-28T07:21:53.907316Z"
        },
        "trusted": true,
        "id": "82nXG6biw05t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Инициализация модели и её обучение"
      ],
      "metadata": {
        "id": "hgDdUfIHw05u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size=512\n",
        "vocab_size = len(dataset.vocab)\n",
        "attention_dim=512\n",
        "encoder_dim=2048\n",
        "decoder_dim=512\n",
        "learning_rate = 3e-4"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:53.909349Z",
          "iopub.execute_input": "2022-04-28T07:21:53.909741Z",
          "iopub.status.idle": "2022-04-28T07:21:53.928881Z",
          "shell.execute_reply.started": "2022-04-28T07:21:53.909697Z",
          "shell.execute_reply": "2022-04-28T07:21:53.928216Z"
        },
        "trusted": true,
        "id": "w4tWFCrww05u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = EncoderDecoder(\n",
        "    embed_size=embed_size,\n",
        "    vocab_size = len(dataset.vocab),\n",
        "    attention_dim=attention_dim,\n",
        "    encoder_dim=encoder_dim,\n",
        "    decoder_dim=decoder_dim\n",
        ").to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:21:53.930237Z",
          "iopub.execute_input": "2022-04-28T07:21:53.930675Z",
          "iopub.status.idle": "2022-04-28T07:22:06.138972Z",
          "shell.execute_reply.started": "2022-04-28T07:21:53.930639Z",
          "shell.execute_reply": "2022-04-28T07:22:06.138235Z"
        },
        "trusted": true,
        "id": "ZAr4RD0uw05u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('cocoweights/model_weights.pth'))\n",
        "model.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:22:06.140177Z",
          "iopub.execute_input": "2022-04-28T07:22:06.140444Z",
          "iopub.status.idle": "2022-04-28T07:22:09.185051Z",
          "shell.execute_reply.started": "2022-04-28T07:22:06.140397Z",
          "shell.execute_reply": "2022-04-28T07:22:09.184365Z"
        },
        "trusted": true,
        "id": "uYyMuUXDw05v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:22:09.18643Z",
          "iopub.execute_input": "2022-04-28T07:22:09.186701Z",
          "iopub.status.idle": "2022-04-28T07:22:09.193148Z",
          "shell.execute_reply.started": "2022-04-28T07:22:09.186664Z",
          "shell.execute_reply": "2022-04-28T07:22:09.192327Z"
        },
        "trusted": true,
        "id": "rgrsibEjw05v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_original(txt):\n",
        "    answer = []\n",
        "    for token in txt[0].tolist():\n",
        "        if token == 1:\n",
        "            continue\n",
        "        elif token == 0:\n",
        "            break\n",
        "        else:\n",
        "            answer.append(dataset.vocab.itos[token])\n",
        "    return answer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:22:09.198012Z",
          "iopub.execute_input": "2022-04-28T07:22:09.198312Z",
          "iopub.status.idle": "2022-04-28T07:22:09.204013Z",
          "shell.execute_reply.started": "2022-04-28T07:22:09.198283Z",
          "shell.execute_reply": "2022-04-28T07:22:09.203262Z"
        },
        "trusted": true,
        "id": "7fQfd4s3w05w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_original_val(txt):\n",
        "    answer = []\n",
        "    for token in txt[0].tolist():\n",
        "        if token == 1:\n",
        "            continue\n",
        "        elif token == 0:\n",
        "            break\n",
        "        else:\n",
        "            answer.append(dataset_val.vocab.itos[token])\n",
        "    return answer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:22:09.205155Z",
          "iopub.execute_input": "2022-04-28T07:22:09.205896Z",
          "iopub.status.idle": "2022-04-28T07:22:09.212619Z",
          "shell.execute_reply.started": "2022-04-28T07:22:09.205858Z",
          "shell.execute_reply": "2022-04-28T07:22:09.211969Z"
        },
        "trusted": true,
        "id": "NG-U3W5Cw05w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "print_every = 300\n",
        "k = 0\n",
        "\n",
        "for epoch in range(1,num_epochs+1):   \n",
        "    for idx, (image, captions) in enumerate(iter(data_loader)):\n",
        "        image,captions = image.to(device),captions.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs,attentions = model(image, captions)\n",
        "        targets = captions[:,1:]\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (idx+1)%print_every == 0:\n",
        "            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                print('----------TRAIN----------')\n",
        "                dataiter = iter(data_loader)\n",
        "                img,txt = next(dataiter)\n",
        "                features = model.encoder(img[0:1].to(device))\n",
        "                caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n",
        "                caption = ' '.join(caps)\n",
        "                reference = gen_original(txt)[:-2] if gen_original(txt)[-2] == '.' else gen_original(txt)[:-1]\n",
        "                print(\"reference:\", reference)\n",
        "                print(\"caption:\", caps[:-2])\n",
        "                BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], caps[:-2], weights=(0.25, 0.25, 0.25, 0.25))\n",
        "                print('BLEU score:', BLEUscore)\n",
        "                show_image(img[0],title=caption)\n",
        "                print(\"----------VALID----------\")\n",
        "                dataiter_val = iter(data_loader_val)\n",
        "                img,txt = next(dataiter_val)\n",
        "                features = model.encoder(img[0:1].to(device))\n",
        "                caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n",
        "                caption = ' '.join(caps)\n",
        "                reference = gen_original_val(txt)[:-2] if gen_original_val(txt)[-2] == '.' else gen_original_val(txt)[:-1]\n",
        "                print(\"reference:\", reference)\n",
        "                print(\"caption:\", caps[:-2])\n",
        "                BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], caps[:-2], weights=(0.25, 0.25, 0.25, 0.25))\n",
        "                print('BLEU score:', BLEUscore)\n",
        "                show_image(img[0],title=caption)\n",
        "                print(\"----------TEST----------\")\n",
        "                dataiter_test = iter(data_loader_test)\n",
        "                img,txt = next(dataiter_test)\n",
        "                features = model.encoder(img[0:1].to(device))\n",
        "                caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n",
        "                caption = ' '.join(caps)\n",
        "                show_image(img[0],title=caption)                \n",
        "                torch.save(model.state_dict(), f'model_weights{k}.pth')\n",
        "                k += 1\n",
        "            model.train()\n",
        "    torch.save(model.state_dict(), 'model_weights.pth')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T07:22:09.214401Z",
          "iopub.execute_input": "2022-04-28T07:22:09.214771Z",
          "iopub.status.idle": "2022-04-28T17:53:43.861464Z",
          "shell.execute_reply.started": "2022-04-28T07:22:09.214731Z",
          "shell.execute_reply": "2022-04-28T17:53:43.857277Z"
        },
        "trusted": true,
        "id": "pX-fqYbaw05w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_weights.pth')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-28T17:53:48.28988Z",
          "iopub.execute_input": "2022-04-28T17:53:48.290148Z",
          "iopub.status.idle": "2022-04-28T17:53:48.915358Z",
          "shell.execute_reply.started": "2022-04-28T17:53:48.290117Z",
          "shell.execute_reply": "2022-04-28T17:53:48.914395Z"
        },
        "trusted": true,
        "id": "3yx-GfsBw05x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}